{"x": ["class Tag:\n    def __init__(self, tag_type, elements):\n        self.tag_type = tag_type\n        self.elements = elements\n\n    def serialize(self, stream):\n        StreamIO.write_ubyte(stream, self.tag_type.TYPE_ID)\n        StreamIO.write_int(stream, len(self.elements))\n        for element in self.elements:\n            self.tag_type.write(stream, element)\n\n    @staticmethod\n    def deserialize(stream, tag_type):\n        tag_type_id = StreamIO.read_ubyte(stream)\n        if tag_type_id != tag_type.TYPE_ID:\n            raise ValueError(\"Invalid tag type ID\")\n        num_elements = StreamIO.read_int(stream)\n        elements = [tag_type.read(stream) for _ in range(num_elements)]\n        return Tag(tag_type, elements)", "import requests\nfrom typing import Any, Dict, Tuple\nimport time\n\n\ndef make_post_request(url: str, data: Any, headers: Dict[str, str], data_format: str, retries: int = 5) -> Tuple[int, Any]:\n    if data_format not in ('json', 'xml', 'text'):\n        raise ValueError(\"Invalid data format, please choose 'json', 'xml', or 'text'\")\n\n    if data_format == 'json':\n        headers['Content-Type'] = 'application/json'\n        try:\n            import json\n            data_payload = json.dumps(data)\n        except ValueError as e:\n            raise ValueError(f\"Failed to convert JSON data: {e}\")\n\n    elif data_format == 'xml':\n        headers['Content-Type'] = 'application/xml'\n        # Assuming data is already an XML string\n        data_payload = data\n\n    else:\n        headers['Content-Type'] = 'text/plain'\n        data_payload = data\n\n    delay = 1\n    for i in range(retries + 1):\n        try:\n            response = requests.post(url, data=data_payload, headers=headers)\n            response.raise_for_status()  # Raise exception for bad status codes\n            return response.status_code, response.content\n\n        except (requests.RequestException, requests.Timeout) as e:\n            if i == retries:\n                raise ValueError(\"POST request failed after maximum retries\")\n            else:\n                time.sleep(delay)\n                delay *= 2\n                continue\n        except Exception as e:\n            raise e\n\n# Example usage\nurl = 'https://jsonplaceholder.typicode.com/posts'\npost_data = {\"title\": \"Hello\", \"body\": \"World\", \"userId\": 1}\nheaders = {\"Authorization\": \"Bearer token\"}\n\nstatus, response_content = make_post_request(url, post_data, headers, data_format='json')\n\nprint(f\"Status code: {status}\")\nprint(f\"Response content: {response_content}\")", "def has_unique_chars(string):\n    if len(string) > 128:  # ASCII has only 128 characters\n        return False\n\n    char_set = [False] * 128\n    for char in string:\n        if char_set[ord(char)]:\n            return False\n        char_set[ord(char)] = True\n\n    return True", "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# 1. Load the dataset\ndataset = pd.read_csv('dataset.csv')\n\n# 2. Handling missing values\ndataset = dataset.dropna()\n\n# 3. Separate input features (X) and target variable (y)\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n# 4. Train the linear regression model\nregressor = LinearRegression()\nregressor.fit(X, y)\n\n# 5. Evaluate model performance using k-fold cross-validation\nscores = cross_val_score(regressor, X, y, cv=10)\n\n# 6. Calculate the mean cross-validation score\nmean_score = scores.mean()\n\n# 7. Print the mean cross-validation score\nprint(\"Mean Cross-Validation Score:\", mean_score)", "import math\n\ndef is_prime(number):\n    if number < 2:\n        return False\n\n    for i in range(2, int(math.sqrt(number)) + 1):\n        if number % i == 0:\n            return False\n\n    return True\n\ndef prime_factors(number):\n    factors = []\n\n    for i in range(2, int(math.sqrt(number)) + 1):\n        while number % i == 0:\n            factors.append(i)\n            number = number // i\n\n    if number > 1:\n        factors.append(number)\n\n    return factors\n\ndef check_prime(number):\n    if is_prime(number):\n        return True, []\n    else:\n        return False, prime_factors(number)", "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Assume 'data' is your dataset and 'target' is the column you are trying to predict\nX = data.drop('target', axis=1)\ny = data['target']\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nrf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Get predictions\npredictions = rf.predict(X_test)\n\n# Evaluate the model\nprint(\"Accuracy: \", accuracy_score(y_test, predictions))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))", "import pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\nfrom sklearn.preprocessing import StandardScaler\nfrom multiprocessing import Pool\n\n# Creating the connection string\nDATABASE = 'your_database_name'\nUSER = 'your_user_name'\nPASSWORD = 'your_password'\nHOST = 'your_host'\nPORT = 'your_port'\n\ndef scale_data(data_frame):\n    scaler = StandardScaler()\n    return pd.DataFrame(scaler.fit_transform(data_frame))\n\ndef parallelize_dataframe(func):\n    df_split = np.array_split(data_frame, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\ndef optimized_func(query_string):\n    connection = psycopg2.connect(database=DATABASE, user=USER, password=PASSWORD, host=HOST, port=PORT)\n    \n    segmented_data = pd.read_sql_query(query_string, connection, chunksize = 10000)\n    \n    final_df = pd.DataFrame()\n    \n    for chunk in segmented_data:\n        chunk = scale_data(chunk)\n        chunk = parallelize_dataframe(chunk)\n        final_df = final_df.append(chunk)\n    \n    connection.close()\n    \n    return final_df", "from datetime import datetime\nimport pytz\n\ndef get_datetime(timezone='local', dt_format='dmy_hms'):\n\n    # Define valid inputs\n    valid_timezones = pytz.all_timezones\n    valid_formats = ['dmy_hms', 'iso8601', 'rfc2822']\n    \n    # Validate timezone\n    if timezone != 'local':\n        if timezone not in valid_timezones:\n            raise ValueError(f'Invalid timezone. Please use one of: {valid_timezones}')\n            \n    # Validate date time format\n    if dt_format not in valid_formats:\n        raise ValueError(f'Invalid date time format. Please use one of: {valid_formats}')\n        \n    # Get current date and time\n    if timezone == 'local':\n        current_datetime = datetime.now()\n    else:\n        current_datetime = datetime.now(pytz.timezone(timezone))\n    \n    # Format date and time\n    if dt_format == 'dmy_hms':\n        str_datetime = current_datetime.strftime('%A, %d %B %Y %I:%M:%S %p')\n    elif dt_format == 'iso8601':\n        str_datetime = current_datetime.isoformat()\n    elif dt_format == 'rfc2822':\n        str_datetime = current_datetime.strftime('%a, %d %b %Y %H:%M:%S %z')\n        \n    print(str_datetime)\n\n# Test   \nget_datetime('Asia/Kolkata', 'iso8601')", "def is_valid_ipv6(ip_str):\n    # Check if the string contains a double colon\n    if ip_str.count('::') > 1:\n        return False\n\n    # Split the string by colons to get the groups\n    groups = ip_str.split(':')\n\n    # Check if the number of groups is correct\n    if len(groups) != 8 and '::' not in groups:\n        return False\n\n    # Iterate over each group\n    for group in groups:\n        # Check if the group is empty and not the double colon\n        if group == '':\n            continue\n\n        # Check if the group is a valid hexadecimal number\n        try:\n            int(group, 16)\n        except ValueError:\n            return False\n\n        # Check if the group is within the range of 0000 to FFFF\n        if len(group) > 4:\n            return False\n\n    return True\n\n# Test the function\nip_str = \"2001:0db8:85a3:0000:0000:8a2e:0370:7334\"\nprint(is_valid_ipv6(ip_str))  # Output: True", "import calendar\n\ndef count_first_sundays(year):\n    count = 0\n    for month in range(1, 13):\n        # Check if the first day of the month is a Sunday\n        if calendar.weekday(year, month, 1) == 6:\n            count += 1\n    return count\n\nprint(count_first_sundays(2021))"], "preds": ["\n\nThis code defines a class `Tag` that represents a structured data element in a binary format, comprising a type identifier and a collection of elements. The class provides methods for serializing and deserializing the tag to and from a binary stream, utilizing a companion class `StreamIO` that handles low-level stream operations. The `Tag` class is designed to be used in a context where data needs to be efficiently stored or transmitted in a compact binary format, such as in a file or network communication protocol. It is intended to be instantiated with a `tag_type` object that defines the type of the tag and a list of `elements` that conform to the tag type. The `serialize` method writes the tag to a binary stream, and the `deserialize` method reconstructs a `Tag` object from a binary stream, validating the tag type ID and deserializing the elements accordingly. The `Tag` class is a self-contained unit that can be used to represent a variety of data structures, making it a flexible building block for more complex data formats or protocols.", "\n\nThis code defines a function, `make_post_request`, which sends a POST request to a specified URL with the provided data and headers, retrying the request up to a specified maximum number of times in case of failure. The function takes four inputs: the URL to send the request to, the data to be sent, a dictionary of headers, and the desired data format (json, xml, or text). The function returns a tuple containing the HTTP status code of the response and the response content. The function is designed to handle JSON, XML, and plain text data formats, and it will retry the request with increasing delays between attempts in case of failure, up to a maximum of five retries. The function is intended for use in a production environment where network connectivity may be unreliable, and it provides a robust way to handle transient network errors. To implement this function, an engineer would need to provide a URL, data to be sent, headers, and the desired data format, and the function will handle the rest, returning the status code and response content of the POST request.", "\n\nThis code implements a function to determine whether a given input string contains any duplicate characters. The function takes a string as input and returns a boolean value indicating whether all characters in the string are unique. The function operates under the assumption that the input string is encoded in ASCII, and thus only contains characters with ASCII values up to 128. The function iterates over the input string, maintaining a boolean array of size 128 to track the presence of each character, and returns False as soon as a duplicate character is encountered. If the function completes the iteration without finding any duplicates, it returns True. This function is intended for use in scenarios where string uniqueness needs to be verified, such as in data validation or string processing applications.", "\n\nThis code implements a linear regression model for predictive analysis, specifically designed to evaluate its performance on a given dataset using k-fold cross-validation. The code is intended for use in data science and machine learning applications where a simple, linear relationship between input features and a target variable is assumed. The code assumes a CSV dataset is provided, where the last column represents the target variable and the remaining columns represent the input features. The code drops any rows containing missing values, trains a linear regression model on the cleaned dataset, and evaluates its performance using 10-fold cross-validation, returning the mean cross-validation score as a measure of model accuracy. The code can be adapted for use with various datasets and feature sets, requiring only modification of the dataset file path and the number of folds for cross-validation.", "\n\nThis code implements a prime number validation and factorization system, consisting of three functions: `is_prime`, `prime_factors`, and `check_prime`. The `is_prime` function determines whether a given integer is a prime number by checking divisibility up to its square root. The `prime_factors` function finds the prime factors of a given integer by iteratively dividing it by increasing integers until the square root of the number, appending each divisor to a list of factors. The `check_prime` function combines these two functions to return a boolean indicating whether the input number is prime, along with a list of its prime factors if it is composite. This system is designed to handle positive integers and can be used in various applications requiring prime number validation and factorization, such as cryptography, number theory, or algorithmic mathematics.", "\n\nThis code implements a basic Random Forest Classifier for binary classification tasks. The high-level purpose of the code is to train a machine learning model to predict a target variable based on a given dataset. The intended use case is to evaluate the model's performance on a test set and provide metrics such as accuracy and confusion matrix. The code takes in a dataset with a target variable and splits it into training and testing sets. It then trains a Random Forest Classifier with 100 trees and a maximum depth of 2, and uses it to make predictions on the test set. The model's performance is evaluated using accuracy score and confusion matrix metrics, which are then printed to the console. This code can be adapted for use in any binary classification problem where a Random Forest Classifier is suitable, and can be modified by changing the hyperparameters of the model, such as the number of trees and maximum depth, to improve its performance.", "\n\nThis code is designed to efficiently scale and process large datasets retrieved from a PostgreSQL database using parallel processing. Its intended use case is for data scientists and analysts who need to perform data preprocessing and scaling on massive datasets. The code connects to a PostgreSQL database, retrieves data in chunks, scales it using StandardScaler from scikit-learn, and applies parallel processing to speed up the scaling process. The optimized_func function takes a SQL query string as input, executes it on the database, and returns a scaled and processed pandas DataFrame. To implement this solution, an engineer would need to replace the placeholders for the database connection string with their own credentials and adjust the number of partitions and cores for optimal performance. The code can be adapted for use with other databases and scaling algorithms by modifying the connection string and the scaling function accordingly.", "\n\nThis code defines a function `get_datetime` that retrieves the current date and time in a specified format and timezone. The function takes two parameters: `timezone` and `dt_format`, which specify the desired timezone and date/time format, respectively. The function validates the input parameters to ensure they are valid, and if not, raises a ValueError with a descriptive error message. It then retrieves the current date and time using the `datetime` module, either in the local timezone or the specified timezone, and formats it according to the specified format. The function supports three date/time formats: 'dmy_hms' (day, month, year, hours, minutes, seconds), 'iso8601' (ISO 8601 format), and 'rfc2822' (RFC 2822 format). The function prints the formatted date and time to the console. The code can be implemented in a production environment by calling the `get_datetime` function with the desired timezone and date/time format as arguments, and handling any raised exceptions.", "\n\nThis code implements a function to validate IPv6 addresses. The function, `is_valid_ipv6`, takes a string representing an IPv6 address as input and returns a boolean value indicating whether the address is valid. The function checks for the following conditions: it must not contain more than one double colon (::), it must have exactly 8 groups separated by colons, and each group must be a valid hexadecimal number between 0000 and FFFF, not exceeding 4 characters in length. The function can be used to validate IPv6 addresses in various applications, such as network configuration, routing, and security protocols. It can be adapted for use in network infrastructure, operating systems, and software applications that require IPv6 address validation.", "\n\nThis code is a function designed to count the number of months in a given year that start on a Sunday. The function takes an integer representing the year as input and returns an integer indicating the count of months that begin on a Sunday. The code utilizes the calendar module to determine the day of the week for the first day of each month in the given year, and increments a counter variable whenever it encounters a Sunday. The function can be adapted for use in various applications, such as calendar planning or scheduling, where knowledge of monthly start dates is necessary. To implement this solution, an engineer would need to import the calendar module, define a function with a year parameter, iterate over each month in the year, and use the calendar module to determine the day of the week for the first day of each month, incrementing a counter as needed."]}