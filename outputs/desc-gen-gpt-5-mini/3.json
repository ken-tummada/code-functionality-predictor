{"x": ["class Tag:\n    def __init__(self, tag_type, elements):\n        self.tag_type = tag_type\n        self.elements = elements\n\n    def serialize(self, stream):\n        StreamIO.write_ubyte(stream, self.tag_type.TYPE_ID)\n        StreamIO.write_int(stream, len(self.elements))\n        for element in self.elements:\n            self.tag_type.write(stream, element)\n\n    @staticmethod\n    def deserialize(stream, tag_type):\n        tag_type_id = StreamIO.read_ubyte(stream)\n        if tag_type_id != tag_type.TYPE_ID:\n            raise ValueError(\"Invalid tag type ID\")\n        num_elements = StreamIO.read_int(stream)\n        elements = [tag_type.read(stream) for _ in range(num_elements)]\n        return Tag(tag_type, elements)", "import requests\nfrom typing import Any, Dict, Tuple\nimport time\n\n\ndef make_post_request(url: str, data: Any, headers: Dict[str, str], data_format: str, retries: int = 5) -> Tuple[int, Any]:\n    if data_format not in ('json', 'xml', 'text'):\n        raise ValueError(\"Invalid data format, please choose 'json', 'xml', or 'text'\")\n\n    if data_format == 'json':\n        headers['Content-Type'] = 'application/json'\n        try:\n            import json\n            data_payload = json.dumps(data)\n        except ValueError as e:\n            raise ValueError(f\"Failed to convert JSON data: {e}\")\n\n    elif data_format == 'xml':\n        headers['Content-Type'] = 'application/xml'\n        # Assuming data is already an XML string\n        data_payload = data\n\n    else:\n        headers['Content-Type'] = 'text/plain'\n        data_payload = data\n\n    delay = 1\n    for i in range(retries + 1):\n        try:\n            response = requests.post(url, data=data_payload, headers=headers)\n            response.raise_for_status()  # Raise exception for bad status codes\n            return response.status_code, response.content\n\n        except (requests.RequestException, requests.Timeout) as e:\n            if i == retries:\n                raise ValueError(\"POST request failed after maximum retries\")\n            else:\n                time.sleep(delay)\n                delay *= 2\n                continue\n        except Exception as e:\n            raise e\n\n# Example usage\nurl = 'https://jsonplaceholder.typicode.com/posts'\npost_data = {\"title\": \"Hello\", \"body\": \"World\", \"userId\": 1}\nheaders = {\"Authorization\": \"Bearer token\"}\n\nstatus, response_content = make_post_request(url, post_data, headers, data_format='json')\n\nprint(f\"Status code: {status}\")\nprint(f\"Response content: {response_content}\")", "def has_unique_chars(string):\n    if len(string) > 128:  # ASCII has only 128 characters\n        return False\n\n    char_set = [False] * 128\n    for char in string:\n        if char_set[ord(char)]:\n            return False\n        char_set[ord(char)] = True\n\n    return True", "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# 1. Load the dataset\ndataset = pd.read_csv('dataset.csv')\n\n# 2. Handling missing values\ndataset = dataset.dropna()\n\n# 3. Separate input features (X) and target variable (y)\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n# 4. Train the linear regression model\nregressor = LinearRegression()\nregressor.fit(X, y)\n\n# 5. Evaluate model performance using k-fold cross-validation\nscores = cross_val_score(regressor, X, y, cv=10)\n\n# 6. Calculate the mean cross-validation score\nmean_score = scores.mean()\n\n# 7. Print the mean cross-validation score\nprint(\"Mean Cross-Validation Score:\", mean_score)", "import math\n\ndef is_prime(number):\n    if number < 2:\n        return False\n\n    for i in range(2, int(math.sqrt(number)) + 1):\n        if number % i == 0:\n            return False\n\n    return True\n\ndef prime_factors(number):\n    factors = []\n\n    for i in range(2, int(math.sqrt(number)) + 1):\n        while number % i == 0:\n            factors.append(i)\n            number = number // i\n\n    if number > 1:\n        factors.append(number)\n\n    return factors\n\ndef check_prime(number):\n    if is_prime(number):\n        return True, []\n    else:\n        return False, prime_factors(number)", "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Assume 'data' is your dataset and 'target' is the column you are trying to predict\nX = data.drop('target', axis=1)\ny = data['target']\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nrf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Get predictions\npredictions = rf.predict(X_test)\n\n# Evaluate the model\nprint(\"Accuracy: \", accuracy_score(y_test, predictions))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))", "import pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\nfrom sklearn.preprocessing import StandardScaler\nfrom multiprocessing import Pool\n\n# Creating the connection string\nDATABASE = 'your_database_name'\nUSER = 'your_user_name'\nPASSWORD = 'your_password'\nHOST = 'your_host'\nPORT = 'your_port'\n\ndef scale_data(data_frame):\n    scaler = StandardScaler()\n    return pd.DataFrame(scaler.fit_transform(data_frame))\n\ndef parallelize_dataframe(func):\n    df_split = np.array_split(data_frame, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\ndef optimized_func(query_string):\n    connection = psycopg2.connect(database=DATABASE, user=USER, password=PASSWORD, host=HOST, port=PORT)\n    \n    segmented_data = pd.read_sql_query(query_string, connection, chunksize = 10000)\n    \n    final_df = pd.DataFrame()\n    \n    for chunk in segmented_data:\n        chunk = scale_data(chunk)\n        chunk = parallelize_dataframe(chunk)\n        final_df = final_df.append(chunk)\n    \n    connection.close()\n    \n    return final_df", "from datetime import datetime\nimport pytz\n\ndef get_datetime(timezone='local', dt_format='dmy_hms'):\n\n    # Define valid inputs\n    valid_timezones = pytz.all_timezones\n    valid_formats = ['dmy_hms', 'iso8601', 'rfc2822']\n    \n    # Validate timezone\n    if timezone != 'local':\n        if timezone not in valid_timezones:\n            raise ValueError(f'Invalid timezone. Please use one of: {valid_timezones}')\n            \n    # Validate date time format\n    if dt_format not in valid_formats:\n        raise ValueError(f'Invalid date time format. Please use one of: {valid_formats}')\n        \n    # Get current date and time\n    if timezone == 'local':\n        current_datetime = datetime.now()\n    else:\n        current_datetime = datetime.now(pytz.timezone(timezone))\n    \n    # Format date and time\n    if dt_format == 'dmy_hms':\n        str_datetime = current_datetime.strftime('%A, %d %B %Y %I:%M:%S %p')\n    elif dt_format == 'iso8601':\n        str_datetime = current_datetime.isoformat()\n    elif dt_format == 'rfc2822':\n        str_datetime = current_datetime.strftime('%a, %d %b %Y %H:%M:%S %z')\n        \n    print(str_datetime)\n\n# Test   \nget_datetime('Asia/Kolkata', 'iso8601')", "def is_valid_ipv6(ip_str):\n    # Check if the string contains a double colon\n    if ip_str.count('::') > 1:\n        return False\n\n    # Split the string by colons to get the groups\n    groups = ip_str.split(':')\n\n    # Check if the number of groups is correct\n    if len(groups) != 8 and '::' not in groups:\n        return False\n\n    # Iterate over each group\n    for group in groups:\n        # Check if the group is empty and not the double colon\n        if group == '':\n            continue\n\n        # Check if the group is a valid hexadecimal number\n        try:\n            int(group, 16)\n        except ValueError:\n            return False\n\n        # Check if the group is within the range of 0000 to FFFF\n        if len(group) > 4:\n            return False\n\n    return True\n\n# Test the function\nip_str = \"2001:0db8:85a3:0000:0000:8a2e:0370:7334\"\nprint(is_valid_ipv6(ip_str))  # Output: True", "import calendar\n\ndef count_first_sundays(year):\n    count = 0\n    for month in range(1, 13):\n        # Check if the first day of the month is a Sunday\n        if calendar.weekday(year, month, 1) == 6:\n            count += 1\n    return count\n\nprint(count_first_sundays(2021))"], "preds": ["This code implements a compact, binary serializer/deserializer for a homogeneous typed list (named Tag) intended for use in systems that encode typed collections to a byte stream (e.g., game save formats or custom binary protocols). The Tag object holds a tag_type descriptor and a Python list of elements; tag_type is expected to be a class/object that exposes a unique integer TYPE_ID and two methods or functions write(stream, value) and read(stream) that perform binary serialization of a single element. serialize(stream) writes a 1-byte type identifier (TYPE_ID via StreamIO.write_ubyte), a 4-byte element count (StreamIO.write_int), then serializes each element in order by calling tag_type.write(stream, element). deserialize(stream, tag_type) validates the stream's type id against the provided tag_type, reads the 4-byte element count, then reconstructs the list by calling tag_type.read(stream) repeatedly and returns a Tag(tag_type, elements). The design assumes a StreamIO helper with read_ubyte/read_int/write_ubyte/write_int and consistent endianness/size semantics; implementers should ensure TYPE_ID values are unique, validate non-negative and reasonable element counts to avoid OOM or DoS, handle stream exhaustion/errors robustly, and optionally extend deserialize to map a read TYPE_ID to a tag_type dynamically (rather than requiring the caller to pass tag_type) if you need polymorphic decoding.", "This module implements a small resilient HTTP POST helper intended for services or scripts that need to send JSON, XML, or plain-text POST payloads with automatic retries and exponential backoff; an engineer can reimplement it as a function make_post_request(url: str, data: Any, headers: Dict[str,str], data_format: str, retries: int = 5) -> Tuple[int, Any] that validates data_format \u2208 {'json','xml','text'}, sets the appropriate Content-Type header, prepares a request payload (JSON via json.dumps with conversion error handling, XML assumed to already be a string, text passed through), and issues requests.post calls using the requests library. The function uses response.raise_for_status() so HTTP 4xx/5xx are treated as errors, catches network-related exceptions (requests.RequestException and requests.Timeout) and retries up to retries times with an exponential backoff starting at 1 second and doubling on each retry, returning (response.status_code, response.content) on success; on final failure it raises a ValueError indicating the POST failed after maximum retries, while any other unexpected exception is re-raised. Practical implementation notes: avoid mutating the caller's headers in-place (copy headers before modification), consider returning parsed JSON or response.text if desired, and adjust retry/backoff policy (max attempts, base delay, jitter) to suit production needs.", "This function checks whether all characters in a Python string are unique under the assumption of standard 7-bit ASCII; it accepts one input (a Python str) and returns a boolean: True when every character appears at most once and False otherwise. The algorithm first uses a quick pigeonhole test\u2014if the string length exceeds the charset size (128) it immediately returns False\u2014then iterates the string once, mapping each character to its integer code point via ord() and using a fixed-size boolean array of length 128 to mark seen characters; if a character\u2019s slot is already True the function returns False (duplicate found), otherwise it sets the slot and continues until completion and returns True. This yields O(n) time complexity and O(1) extra space (constant 128 booleans). Important implementation notes and adaptations: the current design assumes all characters are in the 0\u2013127 ASCII range and will error or behave incorrectly for characters with ord() >= 128; to make it robust either validate/encode the input to ASCII (e.g., try s.encode('ascii')) or switch to a set-based approach (seen = set(); check membership and add) to support full Unicode at O(n) time and O(n) space, or expand the boolean array to 256 for extended ASCII/byte inputs. The core components to reimplement are the early length-vs-charset-size check, ord-to-index mapping, fixed-size boolean seen table, single-pass iteration with early exit on duplicate, and returning a boolean result.", "This script is a minimal pipeline to train and evaluate an ordinary least-squares linear regression model on a tabular CSV dataset: it expects a file named \"dataset.csv\" where the last column is the numeric target and all preceding columns are input features (all must be numeric or pre-encoded). Implementation steps are: load the CSV with pandas, drop any rows with missing values (replaceable with imputation for production), split features X = all columns except last and target y = last column, instantiate sklearn.linear_model.LinearRegression, and evaluate model generalization with 10\u2011fold cross\u2011validation via sklearn.model_selection.cross_val_score (the default scoring for regressors is R\u00b2 and cv=10 uses KFold without shuffling); the code computes and prints the mean cross\u2011validation score. Practical adaptation notes: ensure proper preprocessing (categorical encoding, scaling if needed), consider imputation instead of dropna, choose a different scoring metric via the scoring argument (for example 'neg_mean_squared_error'), supply a custom KFold with shuffle=True and a random_state for reproducible splits, and consider regularized estimators (Ridge/Lasso) if multicollinearity or overfitting are concerns; also the explicit regressor.fit call before cross_val_score is unnecessary because cross_val_score fits internally.", "This module provides a small integer primality and prime-factorization utility implemented using trial division: three functions\u2014is_prime(n), prime_factors(n), and check_prime(n)\u2014operate on integer inputs. is_prime returns a boolean indicating whether n is prime (treating any n < 2 as non-prime) by testing divisibility for i in 2..floor(sqrt(n)) and short-circuiting on the first divisor found (complexity O(sqrt(n))). prime_factors returns a list of prime factors (including repeated factors, e.g., 12 -> [2,2,3]) by iteratively dividing out each divisor in the same range and appending the divisor each time it divides evenly, then appending any leftover prime > 1; prime_factors(1) yields an empty list. check_prime combines the two: it returns a tuple (is_prime_bool, factors_list) where the list is empty for primes and contains the full factor multiset for composites. Key implementation details to reproduce: compute the upper trial bound as int(math.sqrt(n)) + 1, use modulus and integer division to extract repeated factors, and handle the final remainder >1. Practical adaptations include validating input type/sign, reducing trial work by handling 2 separately and iterating only odd divisors, using wheel factorization or probabilistic/heuristic algorithms (Miller\u2013Rabin, Pollard's Rho) for large integers, and documenting complexity and integer-size limits if used in performance-sensitive contexts.", "This code is a minimal end-to-end example of training and evaluating a Random Forest classification model with scikit-learn: it expects a pandas DataFrame named data containing a column 'target' (the label) and uses the remaining columns as features (X = data.drop('target')). The workflow is: split features and labels, create a reproducible train/test split (train_test_split with test_size=0.2 and random_state=42), instantiate a RandomForestClassifier (here n_estimators=100, max_depth=2, random_state=0), fit the model on the training partition, produce predictions on the held-out test partition, and compute basic performance metrics (accuracy_score and confusion_matrix). Inputs: a cleaned feature matrix and 1D label vector (numerical or encoded categorical labels); outputs: a fitted model object, predicted label array for X_test, a scalar accuracy and a confusion matrix array. Key behaviors and design notes: RandomForest is an ensemble of decision trees (default Gini splitter) and tolerates unscaled features but requires categorical features to be encoded numerically; max_depth=2 yields very shallow trees and may underfit, while n_estimators controls variance reduction and runtime; random_state makes results deterministic. Practical implementation/adaptation recommendations: validate and preprocess data (impute missing values, encode categoricals, scale if using mixed-model pipelines), consider stratify=y for class balance in splitting, use cross-validation or GridSearchCV/RandomizedSearchCV to tune hyperparameters (e.g., n_estimators, max_depth, min_samples_split, max_features), use additional metrics for imbalanced or multiclass tasks (precision/recall/F1, ROC-AUC, classification_report), inspect feature_importances_ for interpretation, persist the trained model (joblib.dump), and wrap preprocessing + estimator in a Pipeline for production use.", "This module is a streaming, chunked ETL-style processor intended to read large result sets from a PostgreSQL database, standardize numeric features, and parallelize per-chunk processing to improve throughput; inputs are a SQL query string and database credentials, and the output is a pandas.DataFrame (or alternatively written-out chunks) containing the scaled numeric data. At a high level: optimized_func(query_string) opens a psycopg2/SQLAlchemy connection, uses pandas.read_sql_query(query_string, connection, chunksize=10000) to iterate the result set in memory-bounded chunks, and for each chunk performs scaling and parallel processing before concatenating results into a final DataFrame and closing the connection. Core components and dependencies: pandas for DataFrame handling, psycopg2 (or SQLAlchemy engine) for DB streaming, scikit-learn StandardScaler for feature standardization (note: current code calls fit_transform per chunk which computes per-chunk means \u2014 if global scaling is required use StandardScaler.partial_fit or a two-pass algorithm to compute global mean/var), numpy for splitting arrays, and multiprocessing.Pool to parallelize work. Key behaviors and implementation notes to reproduce or adapt: implement scale_data(df, numeric_cols=None) to select numeric columns, preserve column names and index, and return a DataFrame with scaled numeric columns merged back with any non-numeric columns; implement parallelize_dataframe(data_frame, worker_func, num_partitions=None, num_cores=None) that splits the DataFrame with numpy.array_split into num_partitions (defaulting to min(num_cores or os.cpu_count(), len(df))) and then uses Pool(num_cores) with pool.map(worker_func, splits) (or use a context manager to ensure pool.close/join); worker_func should accept a DataFrame split and return a processed DataFrame (e.g., applying scale_data). Replace per-loop DataFrame.append (deprecated and slow) with collecting chunk results in a list and using pd.concat at the end or persist each processed chunk to disk or back to the DB to avoid OOM for very large datasets. Make parameters configurable: chunk_size (chunksize), num_partitions, num_cores, and scaling mode ('per_chunk' vs 'global'). Handle edge cases: ensure numpy is imported, make DB credentials configurable and secured (not hard-coded), handle non-numeric columns and missing values before scaling, consider using StandardScaler.partial_fit for global scaling across streamed chunks, and measure overhead of process startup vs chunk granularity to choose partitioning. The reimplementation should return the concatenated DataFrame or stream processed chunks to storage and must close DB connections and properly terminate worker pools.", "This code is a small utility function that returns a human- or machine-readable representation of the current date/time for a specified timezone and format; it is intended for quick logging/display use in scripts or services that need localized timestamps. Inputs: timezone (string, default 'local') which must be 'local' or a member of pytz.all_timezones, and dt_format (string, default 'dmy_hms') which must be one of ['dmy_hms','iso8601','rfc2822']. Behavior: it validates inputs and raises ValueError on invalid values, obtains the current datetime either as a naive local datetime (datetime.now()) or as a timezone-aware datetime (datetime.now(pytz.timezone(timezone))), and then formats it according to a small format map: 'dmy_hms' -> \"%A, %d %B %Y %I:%M:%S %p\" (weekday, day, month, year, 12-hour time with AM/PM), 'iso8601' -> datetime.isoformat(), and 'rfc2822' -> \"%a, %d %b %Y %H:%M:%S %z\" (RFC-2822 style including numeric offset). The current implementation prints the formatted string (rather than returning it), and note that isoformat() on naive datetimes omits timezone info while the RFC-2822 formatting needs an aware datetime to emit a numeric offset. Core components/dependencies: Python's datetime and pytz. Implementation/adaptation guidance: reproduce a similar signature and validation logic, map formats to the listed strftime/isoformat outputs, ensure you create timezone-aware datetimes if you need offsets (use datetime.now(tz) or for modern Python prefer zoneinfo.ZoneInfo instead of pytz), return the formatted string rather than printing for easier integration, and avoid exposing the entire pytz.all_timezones in error messages (or provide a shorter error list). The supplied test call demonstrates producing an ISO-8601 timestamp for 'Asia/Kolkata'.", "This code is a boolean validator for IPv6 address strings intended to check whether an input conforms to the canonical hexadecimal 8-group IPv6 format (each group 1\u20134 hex digits, values 0x0000\u20130xFFFF) and to allow the standard zero-compression \"::\" once; it takes a single string input and returns True/False. The intended use is a lightweight on-the-fly validator for user input or configuration checks: it should reject addresses with more than one \"::\", invalid hex groups, groups longer than four hex digits, and ensure the final address represents exactly eight 16-bit groups when the optional \"::\" is expanded. A robust reimplementation should: (1) detect and reject multiple occurrences of \"::\"; (2) split the input carefully\u2014handle \"::\" as a special compression token rather than relying on naive split results\u2014so that empty tokens are only permitted as part of a single compression and not as stray leading/trailing colons; (3) when \"::\" is present, compute the number of omitted groups as 8 \u2212 (number of non-empty groups) and expand or validate accordingly; (4) validate each non-empty group against the regex /^[0-9A-Fa-f]{1,4}$/ and ensure numeric range \u2264 0xFFFF; (5) return False for malformed cases such as wrong total groups after expansion or illegal colon placements; (6) optionally extend to accept IPv4-embedded addresses or scope identifiers if required. Complexity is linear in the input length; in practice prefer explicit handling of the compression case (split on \"::\" into head/tail, verify combined group counts \u2264 8, then validate each segment) to avoid the pitfalls in the original implementation that checks for '::' in the list returned by a colon split.", "This Python utility implements a small calendar analysis: given a Gregorian year (an integer input), it counts how many months in that year begin on a Sunday and returns that count (the script prints the integer for a hard-coded year of 2021). Its core design is a simple iteration over months 1\u201312 where each month's first-day weekday is obtained from Python's calendar.weekday(year, month, 1) (which uses Monday=0 \u2026 Sunday=6) and compared against the Sunday code (6); a running integer counter is incremented for matches and returned. Intended use cases include scheduling tools, calendar analytics, validation tests, or components that need to detect month-start weekdays; the routine is O(1) time (12 checks) and trivial memory. Implementation/adaptation notes: validate year input (e.g., 1\u20139999 if using datetime/calendar), parameterize the target weekday or month range if you need e.g. months in a date range or a different first-weekday convention, or replace calendar.weekday with datetime.date(year, month, 1).weekday() (same numbering) for identical behavior; edge cases such as proleptic Gregorian handling and time zones are irrelevant since only calendar dates are used."]}