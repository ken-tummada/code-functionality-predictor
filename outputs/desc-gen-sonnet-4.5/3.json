{"x": ["class Tag:\n    def __init__(self, tag_type, elements):\n        self.tag_type = tag_type\n        self.elements = elements\n\n    def serialize(self, stream):\n        StreamIO.write_ubyte(stream, self.tag_type.TYPE_ID)\n        StreamIO.write_int(stream, len(self.elements))\n        for element in self.elements:\n            self.tag_type.write(stream, element)\n\n    @staticmethod\n    def deserialize(stream, tag_type):\n        tag_type_id = StreamIO.read_ubyte(stream)\n        if tag_type_id != tag_type.TYPE_ID:\n            raise ValueError(\"Invalid tag type ID\")\n        num_elements = StreamIO.read_int(stream)\n        elements = [tag_type.read(stream) for _ in range(num_elements)]\n        return Tag(tag_type, elements)", "import requests\nfrom typing import Any, Dict, Tuple\nimport time\n\n\ndef make_post_request(url: str, data: Any, headers: Dict[str, str], data_format: str, retries: int = 5) -> Tuple[int, Any]:\n    if data_format not in ('json', 'xml', 'text'):\n        raise ValueError(\"Invalid data format, please choose 'json', 'xml', or 'text'\")\n\n    if data_format == 'json':\n        headers['Content-Type'] = 'application/json'\n        try:\n            import json\n            data_payload = json.dumps(data)\n        except ValueError as e:\n            raise ValueError(f\"Failed to convert JSON data: {e}\")\n\n    elif data_format == 'xml':\n        headers['Content-Type'] = 'application/xml'\n        # Assuming data is already an XML string\n        data_payload = data\n\n    else:\n        headers['Content-Type'] = 'text/plain'\n        data_payload = data\n\n    delay = 1\n    for i in range(retries + 1):\n        try:\n            response = requests.post(url, data=data_payload, headers=headers)\n            response.raise_for_status()  # Raise exception for bad status codes\n            return response.status_code, response.content\n\n        except (requests.RequestException, requests.Timeout) as e:\n            if i == retries:\n                raise ValueError(\"POST request failed after maximum retries\")\n            else:\n                time.sleep(delay)\n                delay *= 2\n                continue\n        except Exception as e:\n            raise e\n\n# Example usage\nurl = 'https://jsonplaceholder.typicode.com/posts'\npost_data = {\"title\": \"Hello\", \"body\": \"World\", \"userId\": 1}\nheaders = {\"Authorization\": \"Bearer token\"}\n\nstatus, response_content = make_post_request(url, post_data, headers, data_format='json')\n\nprint(f\"Status code: {status}\")\nprint(f\"Response content: {response_content}\")", "def has_unique_chars(string):\n    if len(string) > 128:  # ASCII has only 128 characters\n        return False\n\n    char_set = [False] * 128\n    for char in string:\n        if char_set[ord(char)]:\n            return False\n        char_set[ord(char)] = True\n\n    return True", "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# 1. Load the dataset\ndataset = pd.read_csv('dataset.csv')\n\n# 2. Handling missing values\ndataset = dataset.dropna()\n\n# 3. Separate input features (X) and target variable (y)\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n# 4. Train the linear regression model\nregressor = LinearRegression()\nregressor.fit(X, y)\n\n# 5. Evaluate model performance using k-fold cross-validation\nscores = cross_val_score(regressor, X, y, cv=10)\n\n# 6. Calculate the mean cross-validation score\nmean_score = scores.mean()\n\n# 7. Print the mean cross-validation score\nprint(\"Mean Cross-Validation Score:\", mean_score)", "import math\n\ndef is_prime(number):\n    if number < 2:\n        return False\n\n    for i in range(2, int(math.sqrt(number)) + 1):\n        if number % i == 0:\n            return False\n\n    return True\n\ndef prime_factors(number):\n    factors = []\n\n    for i in range(2, int(math.sqrt(number)) + 1):\n        while number % i == 0:\n            factors.append(i)\n            number = number // i\n\n    if number > 1:\n        factors.append(number)\n\n    return factors\n\ndef check_prime(number):\n    if is_prime(number):\n        return True, []\n    else:\n        return False, prime_factors(number)", "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Assume 'data' is your dataset and 'target' is the column you are trying to predict\nX = data.drop('target', axis=1)\ny = data['target']\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nrf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Get predictions\npredictions = rf.predict(X_test)\n\n# Evaluate the model\nprint(\"Accuracy: \", accuracy_score(y_test, predictions))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))", "import pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\nfrom sklearn.preprocessing import StandardScaler\nfrom multiprocessing import Pool\n\n# Creating the connection string\nDATABASE = 'your_database_name'\nUSER = 'your_user_name'\nPASSWORD = 'your_password'\nHOST = 'your_host'\nPORT = 'your_port'\n\ndef scale_data(data_frame):\n    scaler = StandardScaler()\n    return pd.DataFrame(scaler.fit_transform(data_frame))\n\ndef parallelize_dataframe(func):\n    df_split = np.array_split(data_frame, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\ndef optimized_func(query_string):\n    connection = psycopg2.connect(database=DATABASE, user=USER, password=PASSWORD, host=HOST, port=PORT)\n    \n    segmented_data = pd.read_sql_query(query_string, connection, chunksize = 10000)\n    \n    final_df = pd.DataFrame()\n    \n    for chunk in segmented_data:\n        chunk = scale_data(chunk)\n        chunk = parallelize_dataframe(chunk)\n        final_df = final_df.append(chunk)\n    \n    connection.close()\n    \n    return final_df", "from datetime import datetime\nimport pytz\n\ndef get_datetime(timezone='local', dt_format='dmy_hms'):\n\n    # Define valid inputs\n    valid_timezones = pytz.all_timezones\n    valid_formats = ['dmy_hms', 'iso8601', 'rfc2822']\n    \n    # Validate timezone\n    if timezone != 'local':\n        if timezone not in valid_timezones:\n            raise ValueError(f'Invalid timezone. Please use one of: {valid_timezones}')\n            \n    # Validate date time format\n    if dt_format not in valid_formats:\n        raise ValueError(f'Invalid date time format. Please use one of: {valid_formats}')\n        \n    # Get current date and time\n    if timezone == 'local':\n        current_datetime = datetime.now()\n    else:\n        current_datetime = datetime.now(pytz.timezone(timezone))\n    \n    # Format date and time\n    if dt_format == 'dmy_hms':\n        str_datetime = current_datetime.strftime('%A, %d %B %Y %I:%M:%S %p')\n    elif dt_format == 'iso8601':\n        str_datetime = current_datetime.isoformat()\n    elif dt_format == 'rfc2822':\n        str_datetime = current_datetime.strftime('%a, %d %b %Y %H:%M:%S %z')\n        \n    print(str_datetime)\n\n# Test   \nget_datetime('Asia/Kolkata', 'iso8601')", "def is_valid_ipv6(ip_str):\n    # Check if the string contains a double colon\n    if ip_str.count('::') > 1:\n        return False\n\n    # Split the string by colons to get the groups\n    groups = ip_str.split(':')\n\n    # Check if the number of groups is correct\n    if len(groups) != 8 and '::' not in groups:\n        return False\n\n    # Iterate over each group\n    for group in groups:\n        # Check if the group is empty and not the double colon\n        if group == '':\n            continue\n\n        # Check if the group is a valid hexadecimal number\n        try:\n            int(group, 16)\n        except ValueError:\n            return False\n\n        # Check if the group is within the range of 0000 to FFFF\n        if len(group) > 4:\n            return False\n\n    return True\n\n# Test the function\nip_str = \"2001:0db8:85a3:0000:0000:8a2e:0370:7334\"\nprint(is_valid_ipv6(ip_str))  # Output: True", "import calendar\n\ndef count_first_sundays(year):\n    count = 0\n    for month in range(1, 13):\n        # Check if the first day of the month is a Sunday\n        if calendar.weekday(year, month, 1) == 6:\n            count += 1\n    return count\n\nprint(count_first_sundays(2021))"], "preds": ["This code implements a generic serialization and deserialization system for tagged array data structures, suitable for binary protocol implementations or file format handling. The Tag class encapsulates a homogeneous collection of elements of a specific type, where the type is represented by a tag_type object that defines both a TYPE_ID identifier and methods for reading and writing individual elements. The serialize method writes a binary representation to a stream consisting of three parts: a single unsigned byte representing the tag type ID, a 32-bit integer indicating the number of elements, and then each element serialized sequentially using the tag_type's write method. The deserialize static method performs the inverse operation by reading the type ID from the stream, validating it against the expected tag_type's TYPE_ID, reading the element count, and then deserializing that many elements using the tag_type's read method, returning a new Tag instance. The design assumes the existence of a StreamIO utility class providing low-level read and write operations for primitive types (read_ubyte, write_ubyte, read_int, write_int) and relies on the tag_type parameter being an object with TYPE_ID, read, and write attributes/methods. This pattern is commonly used in formats like NBT (Named Binary Tag) or similar structured binary protocols where type information must be embedded in the serialized data for correct deserialization, and it allows for extensibility by supporting different tag types through polymorphism or a registry pattern.", "This code implements a resilient HTTP POST request utility function designed to handle various data formats with automatic retry logic and exponential backoff. The function accepts a target URL, payload data, HTTP headers dictionary, a data format specification (json, xml, or text), and an optional retry count (defaulting to 5 attempts). Based on the specified data format, it automatically sets the appropriate Content-Type header and serializes the payload accordingly: JSON data is converted using json.dumps, XML is expected as a pre-formatted string, and text is passed through directly. The core behavior includes exponential backoff retry logic starting with a 1-second delay that doubles after each failure, handling both request exceptions and timeouts gracefully. On successful response (any HTTP status that doesn't raise an exception via raise_for_status), it returns a tuple containing the HTTP status code and the raw response content. If all retry attempts are exhausted, it raises a ValueError with a descriptive message. The implementation is intended for scenarios requiring reliable HTTP communication with external APIs where transient network failures are expected, and it provides format flexibility for different API requirements. An engineer reimplementing this would need to ensure proper exception handling hierarchy, implement the exponential backoff pattern correctly, validate input format constraints before processing, and handle the response according to whether status code validation should fail fast or allow non-2xx responses to return normally.", "This function determines whether a given string contains all unique characters using an optimized approach for ASCII-encoded text. It first performs an early rejection check by returning False if the string length exceeds 128, since ASCII has exactly 128 possible characters and any longer string must contain duplicates by the pigeonhole principle. The core algorithm uses a boolean array of size 128 as a lookup table, where each index corresponds to an ASCII character code. As it iterates through the string, it converts each character to its ASCII integer value using ord() and checks if that index in the array is already marked True, indicating the character was seen before; if so, it returns False immediately. Otherwise, it marks that position as True and continues. If the loop completes without finding duplicates, it returns True. The implementation trades memory (a fixed 128-byte array) for time efficiency with O(n) time complexity and O(1) space complexity relative to the character set size, making it suitable for duplicate detection in ASCII strings where performance matters and the character set is constrained.", "This code implements a supervised machine learning pipeline for linear regression with cross-validation evaluation. The primary purpose is to train a linear regression model on tabular data and assess its performance using k-fold cross-validation to estimate generalization capability. The workflow begins by loading a CSV dataset where the last column is assumed to be the continuous target variable and all preceding columns are numeric input features. Missing values are handled through complete case deletion (dropping any rows with null values). The feature matrix X and target vector y are extracted using positional indexing, then a scikit-learn LinearRegression model is instantiated and fitted to the entire cleaned dataset. Performance evaluation employs 10-fold cross-validation, which partitions the data into 10 subsets, iteratively training on 9 folds and validating on the held-out fold to produce 10 performance scores (default metric is R-squared for regression). The mean of these scores provides a robust estimate of model performance that accounts for data variability. The final output is the mean cross-validation score printed to console. Key implementation considerations include ensuring the dataset has no categorical variables (or preprocessing them appropriately), verifying that the target is in the last column, understanding that dropna may significantly reduce dataset size if many missing values exist, and recognizing that this approach trains the final model on all data but only reports cross-validated performance rather than holding out a separate test set. For production use, this code should be extended to include train-test splitting, feature scaling if needed, hyperparameter tuning, and persistence of the trained model.", "This code provides a prime number checking system with factorization capabilities, consisting of three functions that work together to determine if a given integer is prime and, if not, compute its prime factorization. The main entry point is check_prime, which takes an integer as input and returns a tuple containing a boolean indicating primality and a list of prime factors (empty if prime, populated otherwise). The is_prime function implements a trial division algorithm that tests divisibility from 2 up to the square root of the input number, returning False for numbers less than 2 and True only if no divisors are found. The prime_factors function decomposes a composite number into its prime factors by iterating through potential divisors up to the square root, repeatedly dividing out each factor until it no longer divides the number, then appending any remaining factor greater than 1 (which must be prime). The implementation uses integer division and modulo operations for efficiency, and the square root optimization reduces time complexity from O(n) to O(sqrt(n)) for the primality test. To reimplement this, an engineer should maintain the trial division approach for both primality testing and factorization, ensure the factorization loop exhausts each prime factor completely before moving to the next candidate, handle the edge case where the remaining quotient after the loop is itself a prime factor, and return the combined result as a boolean-list tuple for convenient consumption by calling code.", "This code implements a supervised binary or multiclass classification pipeline using a Random Forest ensemble classifier from scikit-learn. The purpose is to train a predictive model on tabular data where a target column represents class labels to be predicted from feature columns. The workflow begins by separating features (all columns except 'target') from the label column, then splitting the dataset into 80% training and 20% testing subsets using a fixed random seed for reproducibility. A Random Forest classifier is instantiated with 100 decision trees, each limited to a maximum depth of 2 to prevent overfitting, and trained on the training subset using the fit method. After training, the model generates predictions on the held-out test set, and performance is evaluated using accuracy score (proportion of correct predictions) and a confusion matrix (cross-tabulation of actual versus predicted class labels). Key technical specifications include the train-test split ratio, the number of estimators, and maximum tree depth, which control model complexity and generalization. To adapt this code, an engineer would need to ensure the input data is a pandas DataFrame with a 'target' column, handle any necessary preprocessing such as encoding categorical variables or scaling features, and potentially tune hyperparameters like n_estimators, max_depth, min_samples_split, or use cross-validation for more robust evaluation. The output consists of printed accuracy and confusion matrix metrics that inform model performance and classification behavior across different classes.", "This code implements a parallelized data processing pipeline that extracts data from a PostgreSQL database, applies standardization preprocessing, and processes it in chunks to handle large datasets efficiently. The primary use case is ETL workflows where large-scale numerical data needs to be retrieved from a database, normalized using StandardScaler from scikit-learn, and processed in parallel to optimize performance. The core components include a PostgreSQL connection via psycopg2, chunked data reading using pandas read_sql_query with a configurable chunksize parameter (set to 10000 rows), and multiprocessing using Python's Pool to distribute work across CPU cores. The scale_data function applies StandardScaler fit_transform to normalize numerical features, while parallelize_dataframe splits a dataframe into partitions matching available CPU cores and applies a given function in parallel using pool.map. The main optimized_func accepts a SQL query string, establishes a database connection, iterates through data chunks, applies scaling to each chunk, parallelizes processing within each chunk, and concatenates results into a final dataframe before closing the connection. Key technical considerations for implementation include defining global variables num_partitions, num_cores, and data_frame which appear referenced but not declared in the provided code, replacing database credentials with actual connection parameters, importing numpy for array_split, and noting that the parallelize_dataframe function references data_frame globally rather than accepting it as a parameter which should be refactored for proper encapsulation. The approach trades memory efficiency through chunking against computational overhead from repeatedly creating scaler instances and pool processes per chunk rather than reusing them across iterations.", "This function retrieves the current date and time and formats it according to specified timezone and format parameters, then prints the result to standard output. It accepts two optional parameters: timezone (defaulting to 'local') which can be either the string 'local' for system local time or any valid pytz timezone identifier from the pytz.all_timezones list, and dt_format (defaulting to 'dmy_hms') which must be one of three supported format strings: 'dmy_hms' for a human-readable format like \"Monday, 01 January 2024 01:30:45 PM\", 'iso8601' for ISO 8601 format using Python's isoformat method, or 'rfc2822' for RFC 2822 compliant format with timezone offset. The function performs input validation by checking if the provided timezone exists in pytz's timezone database (unless 'local' is specified) and if the format string is one of the three allowed values, raising a ValueError with descriptive messages if validation fails. When timezone is 'local', it uses datetime.now() without timezone awareness, otherwise it creates a timezone-aware datetime object using pytz.timezone(). The formatted datetime string is generated using strftime with appropriate format codes for the selected format type, then printed directly rather than returned. For practical implementation, engineers should note that this function has side effects (printing) rather than returning values, the 'local' option produces naive datetime objects while named timezones produce aware objects, and the RFC 2822 format will only include timezone offset information when a non-local timezone is specified since naive datetimes have no offset.", "This function validates whether a given string represents a properly formatted IPv6 address according to standard IPv6 notation rules. The validator accepts a string as input and returns a boolean indicating validity. It enforces several key IPv6 specifications: at most one occurrence of the double-colon shorthand (used to compress consecutive zero groups), exactly eight colon-separated groups when not using compression, each group containing 1-4 hexadecimal characters (0-9, a-f, case insensitive), and valid hexadecimal syntax for all non-empty groups. The implementation splits the input string on colons to extract groups, checks for multiple double-colons which would be invalid, verifies group count requirements (8 groups without compression or variable count with it), and validates each group by attempting hexadecimal integer parsing and length constraints. The function handles empty strings that result from splitting around double-colons by simply continuing iteration. The primary use case is input validation for network configuration systems, address parsers, or any application requiring IPv6 address format verification. A correct reimplementation should note that this validator has a logical flaw: it doesn't properly validate the relationship between group count and double-colon presence (the condition checks if '::' is in the split groups list, which would only be true for edge cases like starting or ending with '::', not for '::' appearing mid-string), so a robust implementation should track whether double-colon compression is present separately and adjust the expected group count accordingly (fewer than 8 groups should be valid when compression is used).", "This code calculates the number of months in a given year where the first day of the month falls on a Sunday. The function accepts a single integer parameter representing a year and returns an integer count. It leverages Python's calendar module, specifically the weekday function which returns an integer representing the day of the week for a given date (where Monday is 0 and Sunday is 6). The implementation iterates through all twelve months of the specified year, checking whether the first day of each month corresponds to Sunday by comparing the weekday return value to 6, and increments a counter whenever this condition is met. The function is stateless and deterministic, producing the same output for the same input year. To adapt this code, one could extend it to accept date ranges spanning multiple years, return which specific months are affected rather than just a count, or modify it to check for different days of the week by changing the comparison value from 6 to the appropriate weekday constant."]}